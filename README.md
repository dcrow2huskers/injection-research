# injection-research
A study comparing injection vulnerabilities in Python and Java code generated by GPT, Gemini, and Claude. Using na√Øve and security-aware prompts, the project applies static and dynamic analysis to identify how often LLMs produce exploitable code and which models or patterns pose the greatest risk.

# üîç Injection Attacks in LLM-Generated Python and Java Code  
### A Comparative Security Study of GPT, Gemini, and Claude

## üìå Overview
This repository contains the research, experimental setup, datasets, and analysis for a study examining **injection vulnerabilities** in **LLM-generated code**. The goal is to understand whether and how modern AI coding tools‚Äî**OpenAI GPT**, **Google Gemini**, and **Anthropic Claude**‚Äîproduce insecure Python and Java code that is susceptible to injection attacks.

The project uses a controlled set of **20 prompts** (naive, security-aware) and applies static + dynamic analysis to measure vulnerability frequency, patterns, and severity.

---

## üéØ Research Goals
- Determine how often each LLM introduces **injection-style vulnerabilities** in generated Python and Java code.
- Compare vulnerability patterns across:
  - **Models:** GPT vs. Gemini vs. Claude  
  - **Languages:** Python vs. Java  
  - **Prompt Types:** naive vs. security-aware 
- Identify the most common vulnerability categories produced by each LLM.
- Evaluate the effectiveness of prompt-based mitigation (secure-prompting).
- Produce recommendations for safe LLM usage in software development environments.

---

## üß™ Methodology

### 1. **Prompt Set**
Prompts fall into two categories:

#### **A. Naive Prompts (10 total)**
These prompts request straightforward code generation with no mention of security.  
Purpose: simulate real users who "just want working code."

#### **B. Security-Aware Prompts (10 total)**
Clear instructions to sanitize input, validate parameters, and prevent injection attacks.  
Purpose: measure whether secure prompting reduces vulnerabilities.

Complete prompt list is stored in:
/prompts/
naive.txt
security_aware.txt

---

### 2. **Models Tested**
- **OpenAI GPT (GPT-5.1)**  
- **Google Gemini (Gemini 3 Pro)**  
- **Anthropic Claude (Sonnet 4.5)**  

Each prompt is run on each model ‚Üí **20 √ó 3 = 60 samples per language**.

---

### 3. **Static Analysis**
Tools include:
- **Bandit** (Python)
- **Semgrep** (Python + Java)
- **SpotBugs (FindSecBugs)** (Java)

Output is stored in: /analysis/static/

---

### 4. **Dynamic Testing**
Where applicable:
- Execute generated code in a sandbox
- Attempt exploitation using test payloads (e.g., command-injection strings, SQL payloads)
- Record which attacks succeed

Output stored in: /analysis/dynamic

---

### 5. **Submitted Files and Directory Structure**

Below is a complete list of submitted files and their formats. 

How to Navigate This Repository
	‚Ä¢	See /code samples/ for all LLM-generated Python and Java programs.
	‚Ä¢	See /analysis/static/ for Bandit, Semgrep, and SpotBugs results.
	‚Ä¢	See /analysis/dynamic/ for runtime artifacts and the dynamic harness.
	‚Ä¢	See /figures/ for all analysis scripts and generated images for the report.

- `/prompts/`
  - `naive.txt` ‚Äì Text file containing the 10 na√Øve prompts used in the study.
  - `security_aware.txt` ‚Äì Text file containing the 10 security-aware prompts.

- `/figures/`
  - `bandit.py`, `bandit_figure.png` ‚Äì Script and generated figure summarizing Bandit static analysis results.
  - `semgrep.py`, `semgrep_figure.png` ‚Äì Script and generated figure visualizing Semgrep findings across languages and prompt types.
  - `spotbugs.py`, `spotbugs_figure.png` ‚Äì Script and generated figure for SpotBugs/FindSecBugs Java bytecode vulnerability categories.
  - `custom_static_analysis.py`, `custom_static_analysis.png` ‚Äì Script and aggregated figure combining results from all static analysis tools.
  - `dynamic.py` ‚Äì Script used to process runtime test logs and produce dynamic analysis visualizations.
  - `dynamic_sankey.html`, `dynamic_sankey.png` ‚Äì Interactive and static Sankey diagrams visualizing model ‚Üí prompt ‚Üí vulnerability outcome flow.

- `/code samples/`
  - `/python/`
    - `/naive/`
      - `/gpt/` ‚Äì Contains `promptX.py` files generated by GPT using na√Øve prompts.
      - `/gemini/` ‚Äì Contains `promptX.py` files generated by Gemini using na√Øve prompts.
      - `/claude/` ‚Äì Contains `promptX.py` files generated by Claude using na√Øve prompts.
    - `/security-aware/`
      - `/gpt/` ‚Äì Contains `promptX.py` files generated by GPT using security-aware prompts.
      - `/gemini/` ‚Äì Contains `promptX.py` files generated by Gemini using security-aware prompts.
      - `/claude/` ‚Äì Contains `promptX.py` files generated by Claude using security-aware prompts.

  - `/java/`
    - `/naive/`
      - `/gpt/` ‚Äì Contains `promptX.java` files generated by GPT using na√Øve prompts.
      - `/gemini/` ‚Äì Contains `promptX.java` files generated by Gemini using na√Øve prompts.
      - `/claude/` ‚Äì Contains `promptX.java` files generated by Claude using na√Øve prompts.
    - `/security-aware/`
      - `/gpt/` ‚Äì Contains `promptX.java` files generated by GPT using security-aware prompts.
      - `/gemini/` ‚Äì Contains `promptX.java` files generated by Gemini using security-aware prompts.
      - `/claude/` ‚Äì Contains `promptX.java` files generated by Claude using security-aware prompts.

- `/analysis/static/`
  - `/java-naive/`
    - `spotbugs_claude_naive.csv` ‚Äì SpotBugs results for Claude‚Äôs na√Øve Java outputs.
    - `spotbugs_gemini_naive.csv` ‚Äì SpotBugs results for Gemini‚Äôs na√Øve Java outputs.
    - `spotbugs_gpt_naive.csv` ‚Äì SpotBugs results for GPT‚Äôs na√Øve Java outputs.

  - `/java-security-aware/`
    - `spotbugs_claude_security_aware.csv` ‚Äì SpotBugs results for Claude‚Äôs security-aware Java outputs.
    - `spotbugs_gemini_security_aware.csv` ‚Äì SpotBugs results for Gemini‚Äôs security-aware Java outputs.
    - `spotbugs_gpt_security_aware.csv` ‚Äì SpotBugs results for GPT‚Äôs security-aware Java outputs.

  - `/python-naive/`
    - `bandit_claude_naive.csv` ‚Äì Bandit results for Claude‚Äôs na√Øve Python outputs.
    - `bandit_gemini_naive.csv` ‚Äì Bandit results for Gemini‚Äôs na√Øve Python outputs.
    - `bandit_gpt_naive.csv` ‚Äì Bandit results for GPT‚Äôs na√Øve Python outputs.

  - `/python-security-aware/`
    - `bandit_claude_security_aware.csv` ‚Äì Bandit results for Claude‚Äôs security-aware Python outputs.
    - `bandit_gemini_security_aware.csv` ‚Äì Bandit results for Gemini‚Äôs security-aware Python outputs.
    - `bandit_gpt_security_aware.csv` ‚Äì Bandit results for GPT‚Äôs security-aware Python outputs.

  - `java_security_report.json` ‚Äì Aggregated Java static analysis summary.
  - `python_security_report.json` ‚Äì Aggregated Python static analysis summary.
  - `semgrep_static_analysis.csv` ‚Äì Combined Semgrep results used for cross-language comparison.

  - `static_test_java.py` ‚Äì Script used to execute or validate Java static analysis tooling during experimentation.
  - `static_test_python.py` ‚Äì Script used to execute or validate Python static analysis tooling during experimentation.

- `/analysis/dynamic/`
  - `/data/`
    - `log.txt` ‚Äì Raw execution log produced during dynamic testing.
  - `documents/`
    - (various `.txt` files) ‚Äì Input files generated or modified during runtime execution.
  - `safe_data/`
    - (various `.txt` files) ‚Äì Sanitized sample files used for controlled execution tests.
  - `config.json` ‚Äì Auto‚Äëgenerated configuration file produced during dynamic harness initialization.
  - `sample.txt`, `demo.txt` ‚Äì Example runtime artifacts created by executed LLM-generated programs.
  - `products.db`, `users.db` ‚Äì Temporary SQLite databases created automatically during dynamic injection testing.
  - `dynamic_security_harness.py` ‚Äì Script used to execute dynamic analysis and run simulated injection attempts.
  - `dynamic_security_report.json` ‚Äì Final aggregated report summarizing all dynamic test outcomes.

- `README.md` ‚Äì This document.

---

### 6. Known Limitations
	‚Ä¢	The generated Python and Java programs are not production-ready and may fail to execute due to syntax errors, missing dependencies, or incomplete logic.
	‚Ä¢	Dynamic testing was performed only on samples that executed safely in a sandbox environment.
	‚Ä¢	Static analysis tools may produce false positives or negatives (a known limitation of Bandit, Semgrep, and SpotBugs).
	‚Ä¢	Results reflect specific model versions at the time of the experiment and may differ for future LLM iterations.
	‚Ä¢	Some artifacts in /analysis/dynamic/ (e.g., .txt, .db, .json files) were created automatically during runtime execution.

---

### 7. Intended Use

> **Important:** This repository is an **archival research artifact**.  
> Artifact graders **do not need to run** the code to evaluate the project.

All source code and analysis artifacts are included so that the experiments **could** be reproduced, but the project was designed as a **measurement study**, not as a user-facing application. Thus, no makefile or compilation instructions are included.

- The Python and Java files under `/code samples/` are **LLM outputs** used as subjects for analysis.
- The scripts in `/figures/` or `/analysis/` were written only to:
  - Invoke custom static/dynamic analysis testing.
  - Visualize results from json and csv analysis output.
- There is **no single ‚Äúmain program‚Äù** to compile or run.

---