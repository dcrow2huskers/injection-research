# injection-research
A study comparing injection vulnerabilities in Python and Java code generated by GPT, Gemini, and Claude. Using na√Øve and security-aware prompts, the project applies static and dynamic analysis to identify how often LLMs produce exploitable code and which models or patterns pose the greatest risk.

# üîç Injection Attacks in LLM-Generated Python and Java Code  
### A Comparative Security Study of GPT, Gemini, and Claude

## üìå Overview
This repository contains the research, experimental setup, datasets, and analysis for a study examining **injection vulnerabilities** in **LLM-generated code**. The goal is to understand whether and how modern AI coding tools‚Äî**OpenAI GPT**, **Google Gemini**, and **Anthropic Claude**‚Äîproduce insecure Python and Java code that is susceptible to injection attacks.

The project uses a controlled set of **20 prompts** (naive, security-aware) and applies static + dynamic analysis to measure vulnerability frequency, patterns, and severity.

---

## üéØ Research Goals
- Determine how often each LLM introduces **injection-style vulnerabilities** in generated Python and Java code.
- Compare vulnerability patterns across:
  - **Models:** GPT vs. Gemini vs. Claude  
  - **Languages:** Python vs. Java  
  - **Prompt Types:** na√Øve vs. security-aware 
- Identify the most common vulnerability categories produced by each LLM.
- Evaluate the effectiveness of prompt-based mitigation (secure-prompting).
- Produce recommendations for safe LLM usage in software development environments.

---

## üß™ Methodology

### 1. **Prompt Set**
Prompts fall into two categories:

#### **A. Na√Øve Prompts (10 total)**
These prompts request straightforward code generation with no mention of security.  
Purpose: simulate real users who "just want working code."

#### **B. Security-Aware Prompts (10 total)**
Clear instructions to sanitize input, validate parameters, and prevent injection attacks.  
Purpose: measure whether secure prompting reduces vulnerabilities.

Complete prompt list is stored in:
/prompts/
naive.txt
security_aware.txt
follow_up.txt

---

### 2. **Models Tested**
- **OpenAI GPT (GPT-5.1)**  
- **Google Gemini (Gemini 3 Pro)**  
- **Anthropic Claude (Sonnet 4.5)**  

Each prompt is run on each model ‚Üí **20 √ó 3 = 60 samples per language**.

---

### 3. **Static Analysis**
Tools include:
- **Bandit** (Python)
- **Semgrep** (Python + Java)
- **SpotBugs (FindSecBugs)** (Java)

Output is stored in: /analysis/static/

---

### 4. **Dynamic Testing**
Where applicable:
- Execute generated code in a sandbox
- Attempt exploitation using test payloads (e.g., command-injection strings, SQL payloads)
- Record which attacks succeed

Output stored in: /analysis/dynamic

---

### 5. **Manual Review**
Code samples will undergo manual review for:
- Injection style
- Mitigation presence
- API misuse
- Severity rating
- Recurring patterns per model

---
