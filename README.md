# injection-research
A study comparing injection vulnerabilities in Python and Java code generated by GPT, Copilot, and Claude. Using na√Øve and security-aware prompts, the project applies static and dynamic analysis to identify how often LLMs produce exploitable code and which models or patterns pose the greatest risk.
